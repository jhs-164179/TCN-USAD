{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:48:32.752656600Z",
     "start_time": "2024-06-10T05:48:30.002992100Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from models import AE, USAD"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\project\\Nurse-ExercisedaysPrediction\\jhs_tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\project\\Nurse-ExercisedaysPrediction\\jhs_tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# mode == (LSTM or GRU)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc3c5c44b01f130e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# test1 AE(AutoEncoder)\n",
    "\n",
    "# input data(3dim, (B, T, C) | C=number of features)\n",
    "x_data = np.random.sample((100, 24, 1)).astype(np.float32)\n",
    "# y_data = np.random.sample((100, 24, 1)).astype(np.float32)\n",
    "\n",
    "# make dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, x_data))\n",
    "dataset = dataset.batch(batch_size=32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:39:17.034282200Z",
     "start_time": "2024-06-10T05:39:16.976167800Z"
    }
   },
   "id": "859376baf62cdf3a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# input_dim = seq_length(T)\n",
    "# z_dim = output of encoder(size of latent vector)\n",
    "# d_hidden_dim = decoder hidden dim\n",
    "autoencoder = AE(24, 32, 64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:39:17.034282200Z",
     "start_time": "2024-06-10T05:39:16.991794700Z"
    }
   },
   "id": "ac260d32306b03c7"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train1_loss: 0.3211 | 1.22 sec\n",
      "epoch 2 train1_loss: 0.2883 | 0.13 sec\n",
      "epoch 3 train1_loss: 0.2395 | 0.14 sec\n",
      "epoch 4 train1_loss: 0.1943 | 0.14 sec\n",
      "epoch 5 train1_loss: 0.1676 | 0.14 sec\n",
      "epoch 6 train1_loss: 0.1440 | 0.14 sec\n",
      "epoch 7 train1_loss: 0.1287 | 0.14 sec\n",
      "epoch 8 train1_loss: 0.1157 | 0.14 sec\n",
      "epoch 9 train1_loss: 0.1065 | 0.14 sec\n",
      "epoch 10 train1_loss: 0.0999 | 0.14 sec\n",
      "epoch 11 train1_loss: 0.0958 | 0.14 sec\n",
      "epoch 12 train1_loss: 0.0928 | 0.14 sec\n",
      "epoch 13 train1_loss: 0.0911 | 0.14 sec\n",
      "epoch 14 train1_loss: 0.0904 | 0.14 sec\n",
      "epoch 15 train1_loss: 0.0895 | 0.14 sec\n",
      "epoch 16 train1_loss: 0.0888 | 0.14 sec\n",
      "epoch 17 train1_loss: 0.0881 | 0.14 sec\n",
      "epoch 18 train1_loss: 0.0876 | 0.12 sec\n",
      "epoch 19 train1_loss: 0.0872 | 0.14 sec\n",
      "epoch 20 train1_loss: 0.0867 | 0.14 sec\n",
      "epoch 21 train1_loss: 0.0863 | 0.14 sec\n",
      "epoch 22 train1_loss: 0.0859 | 0.15 sec\n",
      "epoch 23 train1_loss: 0.0855 | 0.13 sec\n",
      "epoch 24 train1_loss: 0.0852 | 0.14 sec\n",
      "epoch 25 train1_loss: 0.0849 | 0.14 sec\n",
      "epoch 26 train1_loss: 0.0846 | 0.14 sec\n",
      "epoch 27 train1_loss: 0.0844 | 0.14 sec\n",
      "epoch 28 train1_loss: 0.0842 | 0.14 sec\n",
      "epoch 29 train1_loss: 0.0840 | 0.14 sec\n",
      "epoch 30 train1_loss: 0.0839 | 0.14 sec\n",
      "epoch 31 train1_loss: 0.0838 | 0.14 sec\n",
      "epoch 32 train1_loss: 0.0837 | 0.14 sec\n",
      "epoch 33 train1_loss: 0.0836 | 0.14 sec\n",
      "epoch 34 train1_loss: 0.0835 | 0.14 sec\n",
      "epoch 35 train1_loss: 0.0835 | 0.16 sec\n",
      "epoch 36 train1_loss: 0.0834 | 0.12 sec\n",
      "epoch 37 train1_loss: 0.0834 | 0.14 sec\n",
      "epoch 38 train1_loss: 0.0833 | 0.14 sec\n",
      "epoch 39 train1_loss: 0.0833 | 0.14 sec\n",
      "epoch 40 train1_loss: 0.0833 | 0.14 sec\n",
      "epoch 41 train1_loss: 0.0832 | 0.16 sec\n",
      "epoch 42 train1_loss: 0.0832 | 0.13 sec\n",
      "epoch 43 train1_loss: 0.0832 | 0.14 sec\n",
      "epoch 44 train1_loss: 0.0831 | 0.14 sec\n",
      "epoch 45 train1_loss: 0.0831 | 0.14 sec\n",
      "epoch 46 train1_loss: 0.0831 | 0.14 sec\n",
      "epoch 47 train1_loss: 0.0830 | 0.16 sec\n",
      "epoch 48 train1_loss: 0.0830 | 0.13 sec\n",
      "epoch 49 train1_loss: 0.0830 | 0.14 sec\n",
      "epoch 50 train1_loss: 0.0829 | 0.14 sec\n",
      "Train time: 8.0781\n"
     ]
    }
   ],
   "source": [
    "hist = autoencoder.fit(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:39:25.111969100Z",
     "start_time": "2024-06-10T05:39:17.018260600Z"
    }
   },
   "id": "2be87845b0abbdee"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# test2 USAD(UnSupervisedAnomalyDetection)\n",
    "\n",
    "# input data(2dim, (B, T))\n",
    "x_data = np.random.sample((100, 24)).astype(np.float32)\n",
    "# y_data = np.random.sample((100, 24, 1)).astype(np.float32)\n",
    "\n",
    "# make dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, x_data))\n",
    "dataset = dataset.batch(batch_size=32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:39:25.127931Z",
     "start_time": "2024-06-10T05:39:25.111969100Z"
    }
   },
   "id": "e46c6cd091e1de4e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# input_dim = seq_length(T)\n",
    "# z_dim = output of encoder(size of latent vector)\n",
    "# e_hidden_dims = encoder hidden dims(list)\n",
    "# d_hidden_dims = decoder hidden dims(list)\n",
    "usad = USAD(24, 32, [64, 32], [32, 64])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:39:25.158860400Z",
     "start_time": "2024-06-10T05:39:25.127931Z"
    }
   },
   "id": "1701e0e26f9e7b32"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train1_loss: 0.0794 | train2_loss: 0.0789 | 0.28 sec\n",
      "epoch 2 train1_loss: 0.0786 | train2_loss: -0.0003 | 0.14 sec\n",
      "epoch 3 train1_loss: 0.0784 | train2_loss: -0.0265 | 0.14 sec\n",
      "epoch 4 train1_loss: 0.0783 | train2_loss: -0.0395 | 0.12 sec\n",
      "epoch 5 train1_loss: 0.0783 | train2_loss: -0.0474 | 0.16 sec\n",
      "epoch 6 train1_loss: 0.0784 | train2_loss: -0.0526 | 0.12 sec\n",
      "epoch 7 train1_loss: 0.0785 | train2_loss: -0.0565 | 0.16 sec\n",
      "epoch 8 train1_loss: 0.0787 | train2_loss: -0.0595 | 0.12 sec\n",
      "epoch 9 train1_loss: 0.0790 | train2_loss: -0.0620 | 0.16 sec\n",
      "epoch 10 train1_loss: 0.0794 | train2_loss: -0.0641 | 0.12 sec\n",
      "epoch 11 train1_loss: 0.0800 | train2_loss: -0.0660 | 0.14 sec\n",
      "epoch 12 train1_loss: 0.0807 | train2_loss: -0.0679 | 0.14 sec\n",
      "epoch 13 train1_loss: 0.0817 | train2_loss: -0.0698 | 0.14 sec\n",
      "epoch 14 train1_loss: 0.0831 | train2_loss: -0.0719 | 0.14 sec\n",
      "epoch 15 train1_loss: 0.0848 | train2_loss: -0.0743 | 0.14 sec\n",
      "epoch 16 train1_loss: 0.0872 | train2_loss: -0.0771 | 0.14 sec\n",
      "epoch 17 train1_loss: 0.0903 | train2_loss: -0.0805 | 0.14 sec\n",
      "epoch 18 train1_loss: 0.0941 | train2_loss: -0.0844 | 0.14 sec\n",
      "epoch 19 train1_loss: 0.0985 | train2_loss: -0.0888 | 0.14 sec\n",
      "epoch 20 train1_loss: 0.1033 | train2_loss: -0.0933 | 0.14 sec\n",
      "epoch 21 train1_loss: 0.1080 | train2_loss: -0.0976 | 0.14 sec\n",
      "epoch 22 train1_loss: 0.1140 | train2_loss: -0.1030 | 0.14 sec\n",
      "epoch 23 train1_loss: 0.1229 | train2_loss: -0.1113 | 0.14 sec\n",
      "epoch 24 train1_loss: 0.1340 | train2_loss: -0.1218 | 0.14 sec\n",
      "epoch 25 train1_loss: 0.1462 | train2_loss: -0.1336 | 0.14 sec\n",
      "epoch 26 train1_loss: 0.1583 | train2_loss: -0.1452 | 0.14 sec\n",
      "epoch 27 train1_loss: 0.1701 | train2_loss: -0.1565 | 0.14 sec\n",
      "epoch 28 train1_loss: 0.1854 | train2_loss: -0.1714 | 0.14 sec\n",
      "epoch 29 train1_loss: 0.2029 | train2_loss: -0.1887 | 0.14 sec\n",
      "epoch 30 train1_loss: 0.2215 | train2_loss: -0.2073 | 0.14 sec\n",
      "epoch 31 train1_loss: 0.2412 | train2_loss: -0.2269 | 0.14 sec\n",
      "epoch 32 train1_loss: 0.2617 | train2_loss: -0.2474 | 0.14 sec\n",
      "epoch 33 train1_loss: 0.2811 | train2_loss: -0.2667 | 0.14 sec\n",
      "epoch 34 train1_loss: 0.2975 | train2_loss: -0.2831 | 0.14 sec\n",
      "epoch 35 train1_loss: 0.3116 | train2_loss: -0.2973 | 0.14 sec\n",
      "epoch 36 train1_loss: 0.3231 | train2_loss: -0.3088 | 0.14 sec\n",
      "epoch 37 train1_loss: 0.3321 | train2_loss: -0.3180 | 0.14 sec\n",
      "epoch 38 train1_loss: 0.3386 | train2_loss: -0.3247 | 0.14 sec\n",
      "epoch 39 train1_loss: 0.3434 | train2_loss: -0.3298 | 0.14 sec\n",
      "epoch 40 train1_loss: 0.3471 | train2_loss: -0.3337 | 0.14 sec\n",
      "epoch 41 train1_loss: 0.3500 | train2_loss: -0.3368 | 0.14 sec\n",
      "epoch 42 train1_loss: 0.3522 | train2_loss: -0.3394 | 0.14 sec\n",
      "epoch 43 train1_loss: 0.3539 | train2_loss: -0.3413 | 0.14 sec\n",
      "epoch 44 train1_loss: 0.3551 | train2_loss: -0.3428 | 0.14 sec\n",
      "epoch 45 train1_loss: 0.3560 | train2_loss: -0.3439 | 0.14 sec\n",
      "epoch 46 train1_loss: 0.3567 | train2_loss: -0.3449 | 0.14 sec\n",
      "epoch 47 train1_loss: 0.3572 | train2_loss: -0.3457 | 0.14 sec\n",
      "epoch 48 train1_loss: 0.3576 | train2_loss: -0.3464 | 0.14 sec\n",
      "epoch 49 train1_loss: 0.3580 | train2_loss: -0.3471 | 0.14 sec\n",
      "epoch 50 train1_loss: 0.3584 | train2_loss: -0.3477 | 0.14 sec\n",
      "Train time: 7.1562\n"
     ]
    }
   ],
   "source": [
    "hist2 = usad.fit(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:39:32.330731600Z",
     "start_time": "2024-06-10T05:39:25.158860400Z"
    }
   },
   "id": "e7598e771f3ccaf0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# mode == TCN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eab2751654dde7b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# test1 AE(AutoEncoder)\n",
    "\n",
    "# input data(3dim, (B, T, C) | C=number of features)\n",
    "x_data = np.random.sample((100, 24, 1)).astype(np.float32)\n",
    "# y_data = np.random.sample((100, 24, 1)).astype(np.float32)\n",
    "\n",
    "# make dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, x_data))\n",
    "dataset = dataset.batch(batch_size=32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:48:35.752645300Z",
     "start_time": "2024-06-10T05:48:35.049511700Z"
    }
   },
   "id": "4d79c707af451366"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# input_dim = seq_length(T)\n",
    "# z_dim = output of encoder(size of latent vector)\n",
    "# d_hidden_dim = decoder hidden dim\n",
    "# e_hidden_dim = encoder hidden dim(for TCN)\n",
    "# dilations = dilation rates(for TCN | list)\n",
    "autoencoder = AE(24, 32, 64, 64, [1, 2, 4, 8, 16, 23], mode='TCN')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:48:35.895747800Z",
     "start_time": "2024-06-10T05:48:35.752645300Z"
    }
   },
   "id": "599f7d5a56f3acdc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train1_loss: 0.5661 | 5.17 sec\n",
      "epoch 2 train1_loss: 0.2754 | 0.45 sec\n",
      "epoch 3 train1_loss: 0.2011 | 0.44 sec\n",
      "epoch 4 train1_loss: 0.1545 | 0.44 sec\n",
      "epoch 5 train1_loss: 0.1252 | 0.45 sec\n",
      "epoch 6 train1_loss: 0.1073 | 0.44 sec\n",
      "epoch 7 train1_loss: 0.0985 | 0.44 sec\n",
      "epoch 8 train1_loss: 0.0934 | 0.44 sec\n",
      "epoch 9 train1_loss: 0.0897 | 0.45 sec\n",
      "epoch 10 train1_loss: 0.0872 | 0.44 sec\n",
      "epoch 11 train1_loss: 0.0855 | 0.45 sec\n",
      "epoch 12 train1_loss: 0.0837 | 0.45 sec\n",
      "epoch 13 train1_loss: 0.0819 | 0.45 sec\n",
      "epoch 14 train1_loss: 0.0804 | 0.44 sec\n",
      "epoch 15 train1_loss: 0.0788 | 0.45 sec\n",
      "epoch 16 train1_loss: 0.0778 | 0.44 sec\n",
      "epoch 17 train1_loss: 0.0765 | 0.45 sec\n",
      "epoch 18 train1_loss: 0.0753 | 0.44 sec\n",
      "epoch 19 train1_loss: 0.0738 | 0.45 sec\n",
      "epoch 20 train1_loss: 0.0724 | 0.44 sec\n",
      "epoch 21 train1_loss: 0.0706 | 0.44 sec\n",
      "epoch 22 train1_loss: 0.0690 | 0.45 sec\n",
      "epoch 23 train1_loss: 0.0669 | 0.44 sec\n",
      "epoch 24 train1_loss: 0.0650 | 0.44 sec\n",
      "epoch 25 train1_loss: 0.0626 | 0.44 sec\n",
      "epoch 26 train1_loss: 0.0609 | 0.45 sec\n",
      "epoch 27 train1_loss: 0.0588 | 0.44 sec\n",
      "epoch 28 train1_loss: 0.0591 | 0.45 sec\n",
      "epoch 29 train1_loss: 0.0586 | 0.44 sec\n",
      "epoch 30 train1_loss: 0.0593 | 0.44 sec\n",
      "epoch 31 train1_loss: 0.0557 | 0.47 sec\n",
      "epoch 32 train1_loss: 0.0538 | 0.45 sec\n",
      "epoch 33 train1_loss: 0.0507 | 0.52 sec\n",
      "epoch 34 train1_loss: 0.0507 | 0.45 sec\n",
      "epoch 35 train1_loss: 0.0516 | 0.44 sec\n",
      "epoch 36 train1_loss: 0.0506 | 0.45 sec\n",
      "epoch 37 train1_loss: 0.0464 | 0.44 sec\n",
      "epoch 38 train1_loss: 0.0465 | 0.44 sec\n",
      "epoch 39 train1_loss: 0.0448 | 0.44 sec\n",
      "epoch 40 train1_loss: 0.0416 | 0.44 sec\n",
      "epoch 41 train1_loss: 0.0408 | 0.44 sec\n",
      "epoch 42 train1_loss: 0.0397 | 0.45 sec\n",
      "epoch 43 train1_loss: 0.0382 | 0.47 sec\n",
      "epoch 44 train1_loss: 0.0372 | 0.45 sec\n",
      "epoch 45 train1_loss: 0.0384 | 0.47 sec\n",
      "epoch 46 train1_loss: 0.0384 | 0.45 sec\n",
      "epoch 47 train1_loss: 0.0357 | 0.45 sec\n",
      "epoch 48 train1_loss: 0.0350 | 0.44 sec\n",
      "epoch 49 train1_loss: 0.0325 | 0.47 sec\n",
      "epoch 50 train1_loss: 0.0319 | 0.44 sec\n",
      "Train time: 27.1058\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "hist = autoencoder.fit(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:49:03.018255500Z",
     "start_time": "2024-06-10T05:48:35.896812900Z"
    }
   },
   "id": "10187f86dcd90caf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight name: weight_normalization_6/g:0, Trainable: True\n",
      "Weight name: weight_normalization_6/kernel:0, Trainable: True\n",
      "Weight name: weight_normalization_6/bias:0, Trainable: True\n",
      "Weight name: weight_normalization_7/g:0, Trainable: True\n",
      "Weight name: weight_normalization_7/kernel:0, Trainable: True\n",
      "Weight name: weight_normalization_7/bias:0, Trainable: True\n",
      "Weight name: weight_normalization_8/g:0, Trainable: True\n",
      "Weight name: weight_normalization_8/kernel:0, Trainable: True\n",
      "Weight name: weight_normalization_8/bias:0, Trainable: True\n",
      "Weight name: weight_normalization_9/g:0, Trainable: True\n",
      "Weight name: weight_normalization_9/kernel:0, Trainable: True\n",
      "Weight name: weight_normalization_9/bias:0, Trainable: True\n",
      "Weight name: weight_normalization_10/g:0, Trainable: True\n",
      "Weight name: weight_normalization_10/kernel:0, Trainable: True\n",
      "Weight name: weight_normalization_10/bias:0, Trainable: True\n",
      "Weight name: weight_normalization_11/g:0, Trainable: True\n",
      "Weight name: weight_normalization_11/kernel:0, Trainable: True\n",
      "Weight name: weight_normalization_11/bias:0, Trainable: True\n",
      "Weight name: dense_1/kernel:0, Trainable: True\n",
      "Weight name: dense_1/bias:0, Trainable: True\n",
      "Weight name: time_distributed/kernel:0, Trainable: True\n",
      "Weight name: time_distributed/bias:0, Trainable: True\n"
     ]
    }
   ],
   "source": [
    "# To-Do\n",
    "# 00. make TCN-USAD\n",
    "# 01. prepare dataset(LEAD 1.0, EPIC, aihub)\n",
    "# 02. test\n",
    "\n",
    "# # 모델 가중치 학습 가능 여부 확인\n",
    "# for weight in autoencoder.encoder.trainable_weights:\n",
    "#     print(f\"Weight name: {weight.name}, Trainable: {weight.trainable}\")\n",
    "# for weight in autoencoder.decoder.trainable_weights:\n",
    "#     print(f\"Weight name: {weight.name}, Trainable: {weight.trainable}\")\n",
    "# autoencoder.encoder.trainable_weights\n",
    "# autoencoder.decoder.trainable_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T05:46:27.252596500Z",
     "start_time": "2024-06-10T05:46:27.237061Z"
    }
   },
   "id": "ce6d391142169953"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
